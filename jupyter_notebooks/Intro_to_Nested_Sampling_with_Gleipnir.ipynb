{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Nested Sampling with Gleipnir\n",
    "\n",
    "In addition to Gleipnir and its dependencies, you will also need matplotlib installed to run this notebook in whole.\n",
    "\n",
    "## What is Gleipnir?\n",
    "\n",
    "Gleipnir is a python toolkit that provides an easy to use interface for Nested Sampling that is similar to the calibration tools [PyDREAM](https://github.com/LoLab-VU/PyDREAM) and [SimplePSO](https://github.com/LoLab-VU/ParticleSwarmOptimization). Gleipnir has a built-in implementation of the classic Nested Sampling algorithm, and the toolkit provides a common interface to the Nested Sampling implementations MultiNest, PolyChord, dyPolyChord, DNest4, and Nestle. Gleipnir also has some PySB model-specific utilities, including nestedsample_it/NestedSampleIt and HypSelector. \n",
    "\n",
    "Through Nested Sampling simulations, Gleipnir can be used to compute the Bayesian evidence (or marginal likelihood) of models. The Bayesian evidence can in turn be used for model selection; i.e., users can select between competing models and determine which one is best supported by the data. And as a side-effect of the evidence calculation, estimates of the posterior distributions of the parameters can also be generated; therefore, Gleipnir can also be used for Bayesian model calibration. \n",
    "\n",
    "## Egg Carton likelihood model\n",
    "\n",
    "In this tutorial, we will cover the basics of setting up and running Nested Sampling simulations of the Egg Carton likelihood landscape using Gleipnir; this example was adapted from the [pymultinest_demo.py](https://github.com/JohannesBuchner/PyMultiNest/blob/master/pymultinest_demo.py). \n",
    "\n",
    "For the puroposes of this tutorial, we will use Gleipnir's built-in Nested Sampling implementation; we will cover the interfaces to different Nested Samplers available from Gleipnir in a separate tutorial. \n",
    "\n",
    "The Egg Carton likelihood landscape is a common test case for Bayesian sampling schemes. The model is typically two-dimensional (two parameters) and the landscape generated by the likelihood function is a multi-modal egg carton-like shape; see slide 15 of [this pdf](http://www.nbi.dk/~koskinen/Teaching/AdvancedMethodsInAppliedStatistics2016/Lecture14_MultiNest.pdf) for a visualization of the likelihood landscape. The parameters are each defined on \\[0:10pi\\] with uniform priors. \n",
    "\n",
    "Here is the Egg Carton loglikelihood function which returns the natural logarithm of the likelihood for a given parameter vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NumPy\n",
    "import numpy as np\n",
    "# Define the loglikelihood function.\n",
    "def loglikelihood(parameter_vector):\n",
    "    chi = (np.cos(parameter_vector)).prod()\n",
    "    return (2. + chi)**5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampled Parameters\n",
    "\n",
    "Now that we have our loglikelihood function, let's look at how to define parameters for sampling during the Nested Sampling run. The parameters that are sampled are defined by a list of SampledParameter class instances. The SampledParameter class object stores data on the name of the parameter and the parameter's prior probability distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the SampledParameters class.\n",
    "from gleipnir.sampled_parameter import SampledParameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new SampledParameter needs two arguments: a name and a object defining the prior.\n",
    "\n",
    "For priors we can use frozen RV objects from scipy.stats; in special cases you could also write your own prior distribution class objects, but for most purposes scipy.stats distributions will be sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import the uniform distribution.\n",
    "from scipy.stats import uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll create our list sampled parameters.\n",
    "# There are two parameters 'x' and 'y',each with a uniform prior on [0:10pi]. \n",
    "sampled_parameters = list()\n",
    "sampled_parameters.append(SampledParameter(name='x', prior=uniform(loc=0.0,scale=10.0*np.pi)))\n",
    "sampled_parameters.append(SampledParameter(name='y', prior=uniform(loc=0.0,scale=10.0*np.pi)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've defined our list of paramters that are to be sampled and their prior probability distributions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Nested Sampler\n",
    "\n",
    "Now that we have the loglikelihood function and the sampled parameters setup, let's create our Nested Sampler. First, let's import the Nested Sampler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Nested Sampler class object.\n",
    "from gleipnir.nestedsampling import NestedSampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will import the sampling (i.e., routine used to update sample points during the Nested Sampling run) and the stopping criterion (when/how to exit the Nested Sampling run) we want to use, and explicityly set them. These steps are optional, but we will do it here so that we can explicitly pass these input variables to our upcoming instance of NestedSampling as optional keyword arguments in place of the defaults (for demonstration purposes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the sampler we want to use during the Nested Sampling run.\n",
    "# In this case, we'll use a Metropololis Monte Carlo sampler adapted for Nested Sampling.\n",
    "from gleipnir.nestedsampling.samplers import MetropolisComponentWiseHardNSRejection\n",
    "# Import the stopping criterion object. In this case, we'll use a fixed number of iterations.\n",
    "from gleipnir.nestedsampling.stopping_criterion import NumberOfIterations\n",
    "\n",
    "# Initialize the sampler.\n",
    "sampler = MetropolisComponentWiseHardNSRejection(iterations=20, tuning_cycles=1)\n",
    "# Initialize the stopping criterion -- We'll stop after 10*population_size Nested Samping iterations.\n",
    "stopping_criterion = NumberOfIterations(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to consider the size of our active point population size to use during the Nested Sampling run. For this tutorial, we'll start with 1000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Nested Sampling active point population size.\n",
    "population_size = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've defined our loglikelihood function, our list of sampled parameters, the sampler, and the stopping criterion. We're ready to initialize our Nested Sampling object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the Nested Sampler -- Using the MCMC sampler with hard rejection\n",
    "# of likelihood levels is an implementation of the classic Nested Sampling algorithm.\n",
    "NS = NestedSampling(sampled_parameters,\n",
    "                    loglikelihood,\n",
    "                    population_size,\n",
    "                    sampler=sampler,\n",
    "                    stopping_criterion=stopping_criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the NestedSampling instance we call the run function; the run function has an optional keyword argument 'verbose', which can use to turn on verbose outputs to the standard output during the run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Nested Sampling. - The run function returns a tuple with the log_evidence and its error estimate.\n",
    "log_evidence, log_evidence_error = NS.run(verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The natural logarithm of the evidence for the 2-d Egg Carton likelihood model should be approximately 236."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_evidence: 234.90633826692527 +- 0.07043934294984529 \n"
     ]
    }
   ],
   "source": [
    "print(\"log_evidence: {} +- {} \".format(log_evidence, log_evidence_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our estimate of the log_evidence is close to the expected value (~236). If you are interested, you can adjust the Nested Sampling hyperparameters (population size, sampler iterations, and Nested Sampling iterations) to see if you refine the estimate even more. Recall that the Egg Carton likelihood landscape is highly multi-modal, so increasing the population size will probably improve the evidence estimates by improving the sampling of the different modes in the landscape.\n",
    "\n",
    "Besides the log_evidence and its error estimate, we can also get estimates of the evidence and its error, as well as the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evidence: 1.0435817447280387e+102 +- 1.0729794836864213\n",
      "Information: 4.96170103520592 exp(-Information): 0.00700100873572133\n"
     ]
    }
   ],
   "source": [
    "    # Let's print out the direct evidence estimate. \n",
    "    print(\"evidence: {} +- {}\".format(NS.evidence, NS.evidence_error))\n",
    "    # Let's print out the information and exp(-information). \n",
    "    # Note that exp(-information) is an estimate of the fraction of prior space in which the bulk of the\n",
    "    # posterior mass is located; it is a kind of the compression factor for going from prior to posterior.\n",
    "    print(\"Information: {} exp(-Information): {}\".format(NS.information, np.exp(-NS.information)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Posterior marginal distributions\n",
    "\n",
    "Although the Nested Sampling algorithm was developed primarily as a means of estimating the Bayesian evidence (or marginal likelihood), as a by-product of the Nested Sampling run we can also derive estimates of the posterior marginal distributions of the parameters that were sampled during the simulation. With the Gleipnir Nested Sampling classes we can compute these distributions using the posteriors function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the posterior distributions -- the posteriors are returned as a dictionary\n",
    "# keyed to the names of the sampled parameters. Each element is a histogram\n",
    "# estimate of the marginal distribution, including the heights and centers.\n",
    "posteriors = NS.posteriors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at the 'x' parameter. Each parameter has a tuple\n",
    "# containing the histogram estimate of the posterior marginal probability \n",
    "# along with the corresponding bin edges and centers.\n",
    "marginal, edges, centers = posteriors['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now let's plot the distribution using matplotlib.\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.hist(centers, bins=edges, weights=marginal)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
